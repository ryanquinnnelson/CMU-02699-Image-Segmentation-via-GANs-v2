{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e250ee8b-12cc-4f4d-9158-6aab3deb37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb63e12f-bc9a-40cd-b6f9-b4263bf32441",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 1.],\n",
       "        [1., 1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([[0., 0., 1., 1., 1.],\n",
    "       [0., 0., 0., 1., 1.],\n",
    "       [0., 0., 0., 0., 1.],\n",
    "       [1., 0., 0., 0., 1.],\n",
    "       [1., 1., 0., 0., 1.]])\n",
    "print(target.shape)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d641a9d6-31e5-4274-bbef-c2d89c392039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4687, 0.1387, 0.6461, 0.5121, 0.2060],\n",
      "        [0.0882, 0.7572, 0.0049, 0.3728, 0.4143],\n",
      "        [0.8104, 0.5477, 0.8381, 0.5874, 0.2776],\n",
      "        [0.0821, 0.7569, 0.2585, 0.8314, 0.4017],\n",
      "        [0.1114, 0.4795, 0.4871, 0.5231, 0.0732]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.rand(5,5)\n",
    "print(target)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "218eccca-337c-4f1e-8cec-8e5eb09411e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def _apply_transformations(img, target, transforms_list):\n",
    "    \n",
    "    # apply identical transformations to image and target\n",
    "    \n",
    "    if 'RandomVerticalFlip' in transforms_list and random.random() > 0.5:\n",
    "        print('vflip')\n",
    "        img = transforms.functional_pil.vflip(img)\n",
    "        target = transforms.functional_pil.vflip(target)\n",
    "\n",
    "    if 'RandomHorizontalFlip' in transforms_list and random.random() > 0.5:\n",
    "        print('hflip')\n",
    "        img = transforms.functional_pil.hflip(img)\n",
    "        target = transforms.functional_pil.hflip(target)\n",
    "        \n",
    "    if 'RandomCrop' in transforms_list:\n",
    "        tw = 200\n",
    "        th = 200\n",
    "        w,h = img.size\n",
    "        x1 = random.randint(0, w - tw)\n",
    "        y1 = random.randint(0, h - th)\n",
    "        \n",
    "        img = img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "        target = target.crop((x1, y1, x1 + tw, y1 + th))\n",
    "        \n",
    "    return img, target\n",
    "        \n",
    "    \n",
    "\n",
    "class ImageDataset2(Dataset):\n",
    "    \"\"\"\n",
    "    Defines object that represents an image Dataset. Combines target and image into one tensor before performing\n",
    "    transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, targets_dir, dataset_type, transforms_list=[]):\n",
    "        \"\"\"\n",
    "        Initialize ImageDataset.\n",
    "\n",
    "        Args:\n",
    "            img_dir (str): Directory for images\n",
    "            targets_dir (str): Directory for targets related to given images\n",
    "            transform (transformation to perform on both images and targets):\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.targets_dir = targets_dir\n",
    "        self.transforms_list = transforms_list\n",
    "        self.dataset_type = dataset_type\n",
    "\n",
    "        # prepare image list\n",
    "        img_list = os.listdir(img_dir)\n",
    "        img_list.sort()\n",
    "        img_list.remove('.DS_Store')  # remove mac generated files\n",
    "        self.img_list = [os.path.join(img_dir, img_name) for img_name in img_list]\n",
    "        \n",
    "        # prepare target list\n",
    "        # target image name matches image but also includes suffix\n",
    "        self.target_list = [os.path.join(targets_dir, img_name[:-4] + '_anno.bmp') for img_name in img_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # load image and target\n",
    "        img = Image.open(self.img_list[idx]).convert('RGB')\n",
    "        target = Image.open(self.target_list[idx])\n",
    "        \n",
    "        # apply transformations\n",
    "        if self.dataset_type == 'train':\n",
    "            # apply random transformations to image and target for training set only\n",
    "            img, target = _apply_transformations(img, target, self.transforms_list)\n",
    "        else:\n",
    "            # crop the image in a standard way?\n",
    "            pass\n",
    "\n",
    "        # convert to tensors\n",
    "        tensor_img = transforms.ToTensor()(img)\n",
    "        tensor_target = transforms.ToTensor()(target)  # (C, H, W)\n",
    "  \n",
    "        # keep only first channel because all three channels are given the same value\n",
    "        tensor_target_first_channel = tensor_target.squeeze(0)  # (H,W)\n",
    "\n",
    "        # convert all nonzero target values to 1\n",
    "        # nonzero values indicate segment\n",
    "        # zero values indicate background\n",
    "        tensor_target_first_channel[tensor_target_first_channel != 0] = 1.0\n",
    "\n",
    "        # convert target to long datatype to indicate classes\n",
    "        tensor_target_first_channel = tensor_target_first_channel.to(torch.long)\n",
    "\n",
    "        return tensor_img, tensor_target_first_channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "35706e98-9992-40b4-86bc-72218338bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ImageDataset2('/Users/ryanqnelson/Desktop/test/data/warwick/b/','/Users/ryanqnelson/Desktop/test/data/warwick/b_anno/','train',['RandomCrop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dfc9136e-534d-4ea1-acf5-ba73cd22ff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 200, 200]) torch.Size([200, 200])\n"
     ]
    }
   ],
   "source": [
    "tensor_img, tensor_target_first_channel = ids.__getitem__(0)\n",
    "print(tensor_img.shape, tensor_target_first_channel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39135844-ac2d-4b26-9fba-7345595618b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dir = '/Users/ryanqnelson/Desktop/test/data/warwick/training/'\n",
    "img_list = os.listdir(img_dir)\n",
    "img_list.sort()\n",
    "img_list.remove('.DS_Store')  # remove mac generated files\n",
    "img_listT = [os.path.join(img_dir, img_name) for img_name in img_list]\n",
    "# print(img_listT)\n",
    "\n",
    "img_dir = '/Users/ryanqnelson/Desktop/test/data/warwick/a/'\n",
    "img_list = os.listdir(img_dir)\n",
    "img_list.sort()\n",
    "img_list.remove('.DS_Store')  # remove mac generated files\n",
    "img_listA = [os.path.join(img_dir, img_name) for img_name in img_list]\n",
    "# print(img_listA)\n",
    "\n",
    "img_dir = '/Users/ryanqnelson/Desktop/test/data/warwick/b/'\n",
    "img_list = os.listdir(img_dir)\n",
    "img_list.sort()\n",
    "img_list.remove('.DS_Store')  # remove mac generated files\n",
    "img_listB = [os.path.join(img_dir, img_name) for img_name in img_list]\n",
    "# print(img_listB)\n",
    "\n",
    "images = img_listT + img_listA + img_listB\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cc6551e2-ce6d-4132-96e9-0de8eb1d2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def calc_channel_sums(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # convert to tensor\n",
    "    tensor_img = transforms.ToTensor()(img)\n",
    "\n",
    "    # sum pixel values for each channel separately\n",
    "    means_per_channel = torch.mean(tensor_img, dim=[1, 2])\n",
    "    stds_per_channel = torch.std(tensor_img, dim=[1, 2])\n",
    "\n",
    "    return means_per_channel, stds_per_channel\n",
    "\n",
    "\n",
    "def calc_images_means_stds(img_path_list):\n",
    "    means = []\n",
    "    stds = []\n",
    "    for img_path in img_path_list:\n",
    "        means_per_channel, stds_per_channel = calc_channel_sums(img_path)\n",
    "        means.append(means_per_channel)\n",
    "        stds.append(stds_per_channel)\n",
    "\n",
    "    # create single tensor of each result\n",
    "    m = torch.vstack(means)\n",
    "    s = torch.vstack(stds)\n",
    "    \n",
    "    # divide by number of results\n",
    "    channel_means = torch.sum(m,dim=0) / len(means)\n",
    "    channel_stds = torch.sum(s,dim=0) / len(stds)\n",
    "    return channel_means, channel_stds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b6c6d0c2-74a0-4fe8-8305-3a4d91b96d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7853, 0.5147, 0.7834]), tensor([0.1566, 0.2124, 0.1169]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_images_means_stds(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
